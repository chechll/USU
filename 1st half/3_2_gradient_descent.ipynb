{"cells":[{"cell_type":"markdown","metadata":{"tags":["en"],"id":"ZspaA3v_VMqq"},"source":["**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"hide_input":false,"id":"smKKouO4LT2q"},"outputs":[],"source":["#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from sympy.utilities.lambdify import lambdify\n","import sympy as sp"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"7_kAJwviVMq5"},"source":["## Gradient Descent\n","\n","This notebook illustrates the basic principles of gradient descent. Let us recall that gradient descent is an gradient-based iterative optimization method. The optimization starts from a certain initial point $\\mathbf{x}_0$, which is at each step being shifted a little against the direction of the gradient. Since gradient represents the direction of the steepest ascent of a function at the specified point, by going against the direction of the gradient the function will be minimized.\n","\n","The update rule which is applied at every step to compute the next point $\\mathbf{x}_{i+1}$ is as follows:\n","\n","\\begin{equation}\n","\\mathbf{x}_{i+1} = \\mathbf{x}_i - \\gamma \\nabla f(\\mathbf{x}_i)\n","\\end{equation}\n","where $\\nabla f(\\mathbf{x}_i)$ is the gradient of the minimized function and $\\gamma$, usually a small number from interval $( 0, 1 \\rangle$, is the learning rate.\n","\n","### Minimization of a Paraboloid: An Example\n","\n","As an example that illustrates gradient descent we are going to use the minimization of a simple function – a paraboloid according to:\n","\n","\\begin{equation}\n","z = f(x, y) = 2x^2 + y^2\n","\\end{equation}\n","#### Visualization of the Paraboloid\n","\n","As the first step we are going to define and visualize the function:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRAgxhoaLT3Y"},"outputs":[],"source":["def f(x, y):\n","    return 2*x**2 + y**2"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"jPuCxCGOVMq7"},"source":["We are going to generate all combinations of points $x, y$ from a certain range and compute $z = f(x, y)$ for them:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQeU3qWSLT3r"},"outputs":[],"source":["xx, yy = np.mgrid[-10:10.2:0.2, -10:10.2:0.2]\n","zz = f(xx, yy)"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"HAu9jutDVMq8"},"source":["We are going to visualize the results in a 3D plot:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgJwz6wOLT3-"},"outputs":[],"source":["plt.figure()\n","# ax = plt.gca(projection='3d')\n","ax = plt.subplot(projection='3d')\n","ax.plot_surface(xx, yy, zz, cmap='Spectral',\n","                linewidth=0, antialiased=True)\n","ax.set_xlabel(\"x\")\n","ax.set_ylabel(\"y\")\n","ax.set_zlabel(\"z\")"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"Gi4mh0QMVMq_"},"source":["3D plots are notorious for being hard to read because some of their elements tend to overlap with each other. To make our plots more readable we will therefore use 2D contour plots instead of 3D plots:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTW7lOU9LT4X"},"outputs":[],"source":["plt.figure()\n","plt.contour(xx, yy, zz, cmap='Spectral')\n","# both axes at the same scale + create a legend\n","plt.gca().set_aspect('equal')\n","plt.xlabel('x'); plt.ylabel('y')\n","plt.colorbar(label='z')"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"hJx3oU_VVMrB"},"source":["Let's wrap this code in an auxiliary function so that we need not repeat it each time from now on:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwmhWtKaLT4t"},"outputs":[],"source":["def plot_func(xx, yy, f, X=None):\n","    if not X is None:\n","        Xmin, Xmax = X[:, 0].min(), X[:, 0].max()\n","        Ymin, Ymax = X[:, 1].min(), X[:, 1].max()\n","\n","        if (Xmin < xx.min() or Xmax > xx.max() or\n","                Ymin < yy.min() or Ymax > yy.max()):\n","            xx = np.linspace(Xmin, Xmax, 100)\n","            yy = np.linspace(Ymin, Ymax, 100)\n","            xx, yy = np.meshgrid(xx, yy)\n","\n","        plt.scatter(X[:, 0], X[:, 1], zorder=10)\n","\n","    zz = f(xx, yy)\n","    plt.contour(xx, yy, zz, cmap='Spectral')\n","    # both axes at the same scale + create a legend\n","    plt.gca().set_aspect('equal')\n","    plt.xlabel('x'); plt.ylabel('y')\n","    plt.colorbar(label='z')"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"193bDJd5VMrC"},"source":["### Gradient of a Function\n","\n","To be able to minimize function $f(x, y)$ using gradient descent, we need to determine its gradient. Let us recall that gradient $\\nabla f(x, y)$ of function $f(x, y)$ is the vector of its first-order partial derivatives. For a two dimensional function $f(x,y)$:\n","\n","\\begin{equation}\n","\\nabla f(x, y) = \\left(\n","    \\frac{\\partial f}{\\partial x},\n","    \\frac{\\partial f}{\\partial y}\n","\\right)^T.\n","\\end{equation}\n","Let us also recall that our function has the form of $f(x, y) = 2x^2 + y^2$. It is therefore easy to determine the partial derivatives. When computing the partial derivative for $x$, term $y^2$ will be considered a constant and we will only be differentiating $2x^2$. When computing the partial derivative for $y$ it will be vice versa. And so we obtain:\n","\n","\\begin{align}\n","\\frac{\\partial f}{\\partial x} &= 4x \\\\[0.75em]\n","\\frac{\\partial f}{\\partial y} &= 2y\n","\\end{align}\n","Our iterative update rule was expressed in vector form – that is to say, for our function with two arguments $x, y$, vector $\\mathbf{x}$ will be 2-dimensional and it will take the form of $\\mathbf{x} = (x, y)^T$.\n","\n","---\n","#### Task 1: Computing the Gradient\n","\n","**Fill in the blanks in the following cell so that function `grad_f` will return the gradient of function $f(x, y)$ as a vector:**\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["teacher"],"id":"cYVqVyiSVMrD"},"outputs":[],"source":["def grad_f(x, y):\n","    #return the gradient here"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"H8IGG60TVMrD"},"source":["### Visualizing the Gradient\n","\n","As we know, the gradient of a function indicates the direction of its steepest ascent. To get a better idea of what this means we can visualize the gradient, which we have just defined:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owLy5FAyLT5x"},"outputs":[],"source":["xxg, yyg = np.mgrid[-10:11:1.5, -10:11:1.5]\n","gg = np.array(\n","    [[grad_f(x, y) for x, y in zip(rx, ry)]\n","          for rx, ry in zip(xxg, yyg)]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WS5GgEj8LT6E"},"outputs":[],"source":["plt.figure(figsize=[8,8])\n","plot_func(xx, yy, f)\n","plt.quiver(xxg, yyg, gg[..., 0], gg[..., 1])"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"8Fqj3HrXVMrE"},"source":["The arrows indicate the gradient's direction. As we can see, they are all pointing outwards – in the direction the paraboloid increases. The size of the arrows indicates the magnitude of the gradient. The arrows close to the centre are tiny (the derivative at a minimum is zero) and they grow towards the margins: because the function grows faster and faster.\n","\n","### Gradient Descent\n","\n","Now we will continue by applying gradient descent in order to minimize our function. As the first step we will define a few parameters: the number of steps and the learning rate:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5PmPNhjLT6k"},"outputs":[],"source":["num_steps = 20\n","learning_rate = 0.1"]},{"cell_type":"markdown","metadata":{"id":"AhzGD3doVMrJ"},"source":["We will store all the computed points in matrix $X$ so that we can later visualize them. The matrix will have 2 columns – first for $x$ and the second for $y$, so the rows of the matrix represent the points of the gradient descent.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSooss2kLT6t"},"outputs":[],"source":["X = np.zeros((num_steps + 1, 2))"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"9bfrEqakVMrK"},"source":["The initial point can either be selected randomly:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPLJ0uY-LT67"},"outputs":[],"source":["X[0] = np.random.uniform(-10, 10, (2,))"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"1JH2K6p3VMrK"},"source":["or we can opt in for some fixed point, so that we get the same result every time:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCkZxt6HLT7E"},"outputs":[],"source":["X[0] = [-9, -8]"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"MW-9Dz5qVMrK"},"source":["Let us once again recall our iterative update rule for computing the next point:\n","\\begin{equation}\n","\\mathbf{x}_{i+1} = \\mathbf{x}_i - \\gamma \\nabla f(\\mathbf{x}_i)\n","\\end{equation}\n","where $\\gamma$ is the learning rate and $\\nabla f(\\mathbf{x}_i)$ is the gradient of the function we are going to minimize.\n","\n","---\n","#### Task 2: Implementing Gradient Descent\n","\n","**Complete the loop below, so it will iteratively compute the gradient for the given number of steps. Add each point to the matrix $X$:**\n","\n","---\n","Hint: the first row of the matrix contains the initial point, the second row will containt the point after the gradient step etc."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["teacher"],"id":"p71_dREqVMrM"},"outputs":[],"source":["for i in range(num_steps):\n","    # add your code here"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"CKCNM6ShVMrM"},"source":["The computed points can finally be visualized and we can ascertain that they really do converge to the minimum of the paraboloid:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJZ_6w4yLT76"},"outputs":[],"source":["plot_func(xx, yy, f, X)"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"_VDUTjDtVMrN"},"source":["---\n","#### Task 3: Wrapping the Code in a Function\n","\n","Let us now again wrap the codes above for gradient descent into a function so that we can call it repeatedly. Notice, that the function takes a parameter `grad_func`, where you can pass another function that returns the gradient. If you never used this feature of Python please check [this link ](https://www.geeksforgeeks.org/passing-function-as-an-argument-in-python/). Also notice that we use the unpacking operator `*` on `X[i]` to pass each its element as a separate arguemnt to the `grad_func`.\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["teacher"],"id":"r5XuaZC3VMrN"},"outputs":[],"source":["def grad_desc(grad_func, init_point,\n","              num_steps, learning_rate):\n","    X = np.zeros((num_steps + 1, 2))\n","    # add the initial point\n","    # add the for loop\n","    return X"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"HqBplg4CVMrN"},"source":["We can test our new function (notice how we pass the `grad_f`), display the gradient and print the minima using:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jai8XBYqLT9-"},"outputs":[],"source":["X = grad_desc(grad_f, init_point=[-9, -8],\n","              num_steps=20, learning_rate=0.1)\n","plot_func(xx, yy, f, X)\n","print(f\"Found minima at the point: {X[-1]}\")"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"f7Aw76reVMrO"},"source":["### Testing Different Learning Rates\n","\n","To illustrate how things work we will now try doing gradient descent with different learning rates.\n","\n","Let's start with $\\gamma = 0.45$:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldMyRMrjLT-H"},"outputs":[],"source":["X = grad_desc(grad_f, init_point=[-9, -8], num_steps=20,\n","      learning_rate=0.45\n",")\n","plot_func(xx, yy, f, X)\n","\n","print(f\"Found minima at the point: {X[-1]}\")"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"CKU1Q8ntVMrO"},"source":["In the plot above you can notice, that the learning rate is quite high, and therefore the $\\mathbf{x}_i$ \"jumps\" between positive and negative values of x axis till it reaches some minima.\n","\n","For $\\gamma = 0.5$ the algorithm will start to oscillate and it will no longer converge to the minimum:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFwsRSleLT-R","scrolled":true},"outputs":[],"source":["X = grad_desc(grad_f, init_point=[-8,-4], num_steps=2,\n","      learning_rate=0.5\n",")\n","plot_func(xx, yy, f, X)\n","print(f\"Found minima at the point: {X[-1]}\")"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"892aTk3IVMrP"},"source":["With $\\gamma > 0.5$ the algorithm will start to diverge and it will actually keep moving away from the minimum (For a better visualisation we changed the coordinates of the intial point to $[-1, 1]$):\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkZDJ5vNLT-b"},"outputs":[],"source":["X = grad_desc(grad_f, init_point=[1, -1], num_steps=10,\n","      learning_rate=0.528\n",")\n","plot_func(xx, yy, f, X)"]},{"cell_type":"markdown","source":["### Is the gradient really steepest ascent?\n","If you look at the contour plot and the function $f(x,y)$, you may notice that the function looks like increasing faster just in the direction of the $x$ axis. How can we verify that the function increases faster in the direction of gradient than in just the direction of the $x$ axis?\n","\n","We can compare the increase in the direction of gradient to the increase in the direction of $x$ axis for a vector of lenght 1.\n","\n","As a direction for the $x$ axis we can a vector $\\mathbf{a}$ of length 1, i.e., $\\rVert\\mathbf{a}\\lVert_2=1$, which points in the direction of the x axis (you will need to find such vector).\n","We will use a point $v=[1,1]$ as our starting point. Next, we need to define vector $\\mathbf{g}$ that is created by rescaling $\\nabla f(\\mathbf{v})$ to an unit length, so we move from the point the same step size as with $a$. Then we compare if $f(\\mathbf{v} + \\mathbf{g}) > f(\\mathbf{v} + \\mathbf{a})$, i.e., if gradients sends us from point $\\mathbf{v}$ to a larger value of $f$ than the $x$ axis direction $\\mathbf{a}$ for the same step size.\n","\n","---\n","#### Task 4: Verifying steepest ascent\n","Compute $\\mathbf{g}$ (using vector norms), create vector $\\mathbf{a}$ and then verify if $f(\\mathbf{v} + \\mathbf{g}) > f(\\mathbf{v} + \\mathbf{a})$.\n","\n","---"],"metadata":{"id":"f4TbjJk3vw9y"}},{"cell_type":"code","source":["g = np.array([0, 0])\n","a = np.array([0, 0])\n","v = np.array([1, 1])\n","print(f\"Value of f for the step in gradient direction {f(*(v + g))}\")\n","print(f\"Value of f for the step in direction of x axis {a}\")"],"metadata":{"id":"-suse2y1Uef1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Improving the gradient function\n","\n","Below, we have a bit improved gradient descent function.\n","\n","---\n","#### Task 5 (non-mandatory) - stopping criterion\n","\n","---\n","\n","To avoid iterating forever, we can add a stopping criterion to the gradient function. The stopping criterion will break the for loop, if the all the elements of the difference between the updated and the previous step $|\\mathbf{x}_{i+1} - \\mathbf{x}_i|$ are smaller than the tolerance criterion, passed by the `relative_tolerance` parameter. Also, keep only the rows of `X`, that were  reached by the for loop, so we omit the -1's."],"metadata":{"id":"IT7kU1iqnJ6n"}},{"cell_type":"code","source":["def grad_desc_i(grad_func, init_point, num_steps, learning_rate, relative_tolerance=0):\n","    # generalization of X for multiple dimensions\n","    X = np.zeros((num_steps + 1, len(init_point))) - 1\n","    X[0] = init_point\n","\n","    for i in range(num_steps):\n","        diff = learning_rate * grad_func(X[i])\n","        X[i+1] = X[i] - diff\n","        # check the tolerance here\n","        # truncate the X vector and break the loop\n","\n","    return X"],"metadata":{"id":"oMlB37h5nQq9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you have noticed, we pass a vector directly to the `grad_func` without unzipping it. Therefore, we will create new gradient function `grad_fv` that will have a vector as an argument.\n","\n","---\n","#### Task 6 - gradient function with vector as parameter\n","Modify the `grad_v` function, so it takes vector as an argument and returns the value of gradient for the provided vector, e.g., instead of `y` you can use `x[1]`.\n","\n","---"],"metadata":{"id":"mGAPuYhmnmv4"}},{"cell_type":"code","source":["def grad_fv(x):\n","    # return the vector"],"metadata":{"id":"XyNH240uVr0N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can check the output of the improved gradient descent function."],"metadata":{"id":"kP1u4f-NXtPy"}},{"cell_type":"code","source":["X = grad_desc_i(grad_fv, init_point=[8, -9], num_steps=100,\n","                learning_rate=0.01, relative_tolerance=0.05)\n","print(f\"minima at point: {X[-1]}\")\n","X.shape"],"metadata":{"id":"ELGLE_BynZvY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## A Multidimensional example for gradient descent\n","Below we have a multidimensional polynomial function. Now, we have a more complicated function $f(x,y,z) = x^2 + y^2 + 3z^2 + xy + 9x - 10y + 20z + 5$. Note, that we use a vector to pass the values of $x, y, z$ arguments."],"metadata":{"id":"tIQbdALDs7ay"}},{"cell_type":"code","source":["def cost_fun(x):\n","  return x[0]**2 + x[1]**2 + 3*x[2]**2 + x[0]*x[1] + 9*x[0] - 10*x[1] + 20*x[2] + 5"],"metadata":{"id":"jfRjd5XZ3DsX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#### Task 7 - computing gradient of a function\n","\n","---\n","\n","Try to compute its gradient on a paper and then modify the a function `grad_fyx` so it will return the gradient."],"metadata":{"id":"MxrCSH1O3EQ8"}},{"cell_type":"code","source":["def grad_xyz(x):\n","  return np.array([\n","      x[0],\n","      x[1],\n","      x[2]])"],"metadata":{"id":"krKoENLO1pDZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below, we run the gradient descent algorithm passing it the gradient function defined above. Print the point at which we found the minima as well as the function value corresponding to the point."],"metadata":{"id":"EvebpGLb3o6u"}},{"cell_type":"code","source":["init_point = np.array([20, -11, 5])\n","X = grad_desc_i(grad_xyz, init_point, 100, 0.005)\n","print(f\"Final point: {np.round(X[-1],3)}, function value: {cost_fun(X[-1]):.4f}\")"],"metadata":{"id":"YofhQzxc36Z_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, you will need to find the minima of the function, but first we need to keep track of the gradient descent's progress. As the minimized function has more dimensions than we can fit to a two dimensional plot, you can print some values of the function's output matrix and corresponding function values to tune the learning rate.\n","\n","---\n","#### Task 8 - printing the progress of the gradient function\n","\n","Complete the `print_grad_progress` function, which prints current point and function value of every `n_skip`-th step of the gradient descent. Also, you can round up the numbers to get a more readable print.\n","\n","---"],"metadata":{"id":"6pyM-4Pw2DYb"}},{"cell_type":"code","source":["def print_grad_progress(X, n_skip, cost_function):\n","    for i in range(1, X.shape[0], n_skip):\n","      # add the formatted print"],"metadata":{"id":"3hc1U9FH3QXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_grad_progress(X, 10, cost_fun)"],"metadata":{"id":"YdMvczHS_sS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","#### Task 9 - finding the minimum of the function\n","\n","Use the `grad_desc_i` and `print_grad_progress` functions to find the minima of the function $f(x,y,x)$ by chagning the learning rate and number of iterations, so you will find the smallest value. What is the value of the minima? What are its coordinates?  \n","\n","---"],"metadata":{"id":"4gk67RbXaMN2"}},{"cell_type":"code","source":["# Find the best value and print the iterations!"],"metadata":{"id":"OgNYRSM9a181"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"deufEltYVMrQ"},"source":["## BONUS - We do not need to know derrivations, we do not need no thoughs control... Computing Symbolic Gradient Automatically\n","\n","When deriving the gradient of our function above we have computed it analytically and then rewrote it as source code by hand. However, in Python it is possible to compute the symbolic gradient automatically – using the `sympy` package. In the following example we will show how this can be done.\n","\n","We will start by defining some symbolic variables that we are going to need – $x$ and $y$ in our case:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEzjTaPkLT-1"},"outputs":[],"source":["symx, symy = sp.symbols('x y')"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"mRl0oCK9VMrQ"},"source":["We will now define function $f(x, y)$ using the symbolic variables:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rl_9lH4LT_C"},"outputs":[],"source":["symf = 2*symx**2 + symy**2\n","symf"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"E3wpahdjVMrR"},"source":["When computing the symbolic gradient we will use a little trick. We will first transform our scalar function into matrix form and then compute its Jacobian. We will get a row vector that corresponds to the gradient as a result:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slggN0K9LT_J"},"outputs":[],"source":["sym_grad_f = sp.Matrix([symf]).jacobian([symx, symy])\n","sym_grad_f"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"oHvE03U0VMrS"},"source":["To be able to use the resulting symbolic representation of gradient to actually compute it for particular values, we'll need to convert it to a standard numeric function. We will also do the same for function $f$:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtl0XF_cLT_P"},"outputs":[],"source":["f = lambdify((symx, symy), symf, \"numpy\")\n","grad_f_sim = lambdify((symx, symy), sym_grad_f, \"numpy\")"]},{"cell_type":"markdown","metadata":{"tags":["en"],"id":"_ucudQCNVMrS"},"source":["We can now apply gradient descent in exactly the same way we did before – but now we no longer need to compute the gradient by hand:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qg0GQFbFLT_b","scrolled":true},"outputs":[],"source":["xx, yy = np.mgrid[-10:10.2:0.2, -10:10.2:0.2]\n","zz = f(xx, yy)\n","\n","X = grad_desc(grad_f_sim, init_point=[-9, -8],\n","              num_steps=20, learning_rate=0.1)\n","plot_func(xx, yy, f, X)"]}],"metadata":{"celltoolbar":"Tags","colab":{"provenance":[{"file_id":"148N2Uk2Gqha28yOjq-mk1NJj68dbrxDl","timestamp":1664993261181}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}