{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dIPOeDwc_6wI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh9LUQmwD3RX"
   },
   "source": [
    "# Prediction performance measures and validation\n",
    "In the cell above, we load all required Python packages for this notebook.\n",
    "In the next cell, we read the data and define the test target vector Note that the training and test sets are provided as separate datasets, so you do not need to perform an explicit train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6dKaWLE0D28y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"https://drive.google.com/uc?id=1-q6hK_8lHoh5YBqUriWDcU-p2ET-Dbb8\")\n",
    "test_df = pd.read_csv(\"https://drive.google.com/uc?id=1PhltOKxh0zwWem64kEnJeL9RykRmsY4_\")\n",
    "\n",
    "test_df['Survived'] = np.array([0,1,0,0,1,0,1,0,1,0,0,0,1,0,1,1,0,0,1,1,0,0,1,0,1,0,1,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,0,1,1,0,0,1,1,0,1,0,1,0,0,1,0,1,0,0,0,0,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,1,0,1,1,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,1,1,0,1,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,1,0,1,0,0,0,0,1,1,0,1,0,1,0,1,0,1,0,1,1,0,1,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,1,1,0,0,0,1,0,1,0,0,1,0,1,1,0,1,1,0,1,1,0,0,1,0,0,1,1,1,0,0,0,0,0,1,1,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,1,0,0,0,0,0,1,1,1,1,1,0,1,0,0,0])\n",
    "# describes the dataframe\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5lXKLc1D6mM"
   },
   "source": [
    "## Titanic Dataset Description\n",
    "\n",
    "Here, we have a dataset about the survivors from the famous ship Titanic. The response variable is $1$ if the passenger survived and $0$ if not.\n",
    "\n",
    "column | description\n",
    "-------------------|------------------\n",
    "survival | Survival\t0 = No, 1 = Yes\n",
    "pclass | Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "sex | Gender\n",
    "Age | Age in years\n",
    "sibsp | # of siblings / spouses aboard the Titanic\n",
    "parch | # of parents / children aboard the Titanic\n",
    "ticket | Ticket number\n",
    "fare | Passenger fare\n",
    "cabin | Cabin number\n",
    "embarked | Port of Embarkation\n",
    "\n",
    "Below, we have an example of how to extract the response variable and feature matrix from the training data. Your task is to extract `y_test` and `X_test` from the `test_df` in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jbwpnoY4mwY9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>418.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>2.265550</td>\n",
       "      <td>30.272590</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>35.627188</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>120.810458</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>14.181209</td>\n",
       "      <td>0.896760</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>55.907576</td>\n",
       "      <td>0.481622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>892.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>996.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1204.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Age       SibSp       Parch  \\\n",
       "count   418.000000  418.000000  332.000000  418.000000  418.000000   \n",
       "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   \n",
       "std     120.810458    0.841838   14.181209    0.896760    0.981429   \n",
       "min     892.000000    1.000000    0.170000    0.000000    0.000000   \n",
       "25%     996.250000    1.000000   21.000000    0.000000    0.000000   \n",
       "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   \n",
       "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   \n",
       "max    1309.000000    3.000000   76.000000    8.000000    9.000000   \n",
       "\n",
       "             Fare    Survived  \n",
       "count  417.000000  418.000000  \n",
       "mean    35.627188    0.363636  \n",
       "std     55.907576    0.481622  \n",
       "min      0.000000    0.000000  \n",
       "25%      7.895800    0.000000  \n",
       "50%     14.454200    0.000000  \n",
       "75%     31.500000    1.000000  \n",
       "max    512.329200    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df.iloc[:, train_df.columns != 'Survived']\n",
    "y_train = train_df[\"Survived\"]\n",
    "\n",
    "# extract the variables below similarly to the example above\n",
    "X_test = train_df.iloc[:, train_df.columns != 'Survived']\n",
    "y_test = train_df[\"Survived\"]\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ8LKT0WNW7f"
   },
   "source": [
    "Let us check the balance of the response variable in the training dataset.\n",
    "\n",
    "1. Is the data balanced? I.e., does the classes have approximately equal count of elements?\n",
    "2. What kind of methods and techniques would you use to handle a highly imbalanced dataset?\n",
    "Print your answers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3I748O-7Nbp0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "rows: 891\n",
      "\tProportion of survived: 0.3838\n",
      "\tProportion of those who did not survive: 0.6162\n",
      "Test data:\n",
      "rows: 418\n",
      "\tProportion of survived: 0.3636\n",
      "\tProportion of those who did not survive: 0.6364\n"
     ]
    }
   ],
   "source": [
    "n_row_train = train_df.shape[0]\n",
    "print(\"Training data:\")\n",
    "print(f\"rows: {n_row_train}\")\n",
    "print(f'\\tProportion of survived: {sum(train_df[\"Survived\"])/n_row_train:.4f}')\n",
    "print(f'\\tProportion of those who did not survive: {sum(1 - train_df[\"Survived\"])/n_row_train:.4f}')\n",
    "\n",
    "# compare the balance with the test dataset similarly to the above\n",
    "n_row_test = test_df.shape[0]\n",
    "print(\"Test data:\")\n",
    "print(f\"rows: {n_row_test}\")\n",
    "print(f'\\tProportion of survived: {sum(test_df[\"Survived\"])/n_row_test:.4f}')\n",
    "# add proportion of those who did not survive\n",
    "print(f'\\tProportion of those who did not survive: {(n_row_test - sum(test_df[\"Survived\"]))/n_row_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKmCGGJUNd9C"
   },
   "source": [
    "In the cell below create and fit dummy classifier that will predict the test data with the most frequent value from the training data, i.e., with `strategy='most_frequent'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "asUAE3iGhLWU"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "y_pred_train = dummy_majority.predict(X_train).astype(bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIS6VgMahDCO"
   },
   "source": [
    "Below, we check the frequency of each predicted value and verify whether any $0$ was predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-gck9JnBiT4h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequencies of values:\n",
      "[[  0 891]]\n",
      "Number of 1's: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"frequencies of values:\\n{np.array(np.unique(y_pred_train, return_counts=True)).T}\")\n",
    "print(f\"Number of 1's: {sum(y_pred_train == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xws09ffcRhDR"
   },
   "source": [
    "## Confusion Matrix and Prediction Error Measures\n",
    "\n",
    "To evaluate binary classifiers, we will use the confusion matrix. Try to fill in the function and create a confusion matrix as specified below:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "TP & FP\\\\\n",
    "FN & TN\n",
    "\\end{pmatrix}.$\n",
    "\n",
    "We recommend using boolean operators and the `np.invert()` function. An example of how to obtain *FP* is `sum((1 - y_real) & y_predicted)`, where `y_real` and `y_predicted` are the vectors of real and predicted values, respectively. The `1 - y_real` is used to negate the values.\n",
    "\n",
    "In the cell below, we compare it with the confusion matrix from `sklearn`, specifying the class labels to correctly identify $1$ as positive and $0$ as negative. You may notice that the matrix is transposed because the rows represent actual values and the columns represent predicted values. Be aware of this, as you can find confusion matrices in both formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuilB2lhSMXC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your confusion matrix:\n",
      " [[  0   0]\n",
      " [342 549]]\n",
      "Transposed sklearn confusion matrix:\n",
      " [[  0   0]\n",
      " [342 549]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# add the missing elements\n",
    "def my_confusion_matrix(y_real, y_predicted):\n",
    "  TP = sum(y_real & y_predicted)\n",
    "  FP = sum(( 1 - y_real) & (y_predicted))\n",
    "  FN = sum((y_real) & (1 - y_predicted))\n",
    "  TN = sum((1 - y_real) & (1 - y_predicted))\n",
    "  ret_list = [\n",
    "   [TP, FP],\n",
    "   [FN, TN]\n",
    "  ]\n",
    "  return np.array(ret_list)\n",
    "\n",
    "print(f'Your confusion matrix:\\n {my_confusion_matrix(y_train, y_pred_train)}')\n",
    "print(f'Transposed sklearn confusion matrix:\\n {confusion_matrix(y_train, y_pred_train, labels = [1, 0]).T}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xiEyWmNScmR"
   },
   "source": [
    "### Error Measures Using Confusion Matrix Elements\n",
    "\n",
    "Try to write the bodies of the functions below. An example implementation for recall is provided for you.\n",
    "To check your code, you can compare your results with the output in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DwHAJhBgirY7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6000\n",
      "recall: 0.8000\n",
      "specificity: 0.4000\n",
      "precision: 0.5714\n",
      "fall-out: 0.6000\n",
      "f-score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,1,1,1,0,0,0,0,0,1])\n",
    "b = np.array([1,0,1,1,1,1,1,0,0,1])\n",
    "\n",
    "# add code for accuracy\n",
    "def my_accuracy(y_real, y_predicted):\n",
    "  return sum(y_real == y_predicted) / len(y_real)\n",
    "\n",
    "print(f'accuracy: { my_accuracy(a, b):.4f}')\n",
    "\n",
    "def my_recall(y_real, y_predicted):\n",
    "  tp = np.sum( (y_real == 1) & (y_predicted == 1))\n",
    "  return tp / (tp + np.sum( (y_real == 1) & (y_predicted == 0)))\n",
    "\n",
    "print(f'recall: { my_recall(a, b):.4f}')\n",
    "\n",
    "# add code for specificity\n",
    "def my_specificity(y_real, y_predicted):\n",
    "  fp = np.sum(( 1 - y_real) & (y_predicted))\n",
    "  tn = sum(( 1 - y_real) & (1 - y_predicted))\n",
    "  return tn / (tn + fp)\n",
    "\n",
    "print(f'specificity: { my_specificity(a, b):.4f}')\n",
    "\n",
    "# add code for precision\n",
    "def my_precision(y_real, y_predicted):\n",
    "  tp = np.sum( (y_real == 1) & (y_predicted == 1))\n",
    "  fp = np.sum(( 1 - y_real) & (y_predicted))\n",
    "  return tp / (tp + fp)\n",
    "\n",
    "print(f'precision: {my_precision(a, b):.4f}')\n",
    "\n",
    "# add code for fall-out\n",
    "def my_fall_out(y_real, y_predicted):\n",
    "  fp = np.sum(( 1 - y_real) & (y_predicted))\n",
    "  tn = sum(( 1 - y_real) & (1 - y_predicted))\n",
    "  return fp / (fp + tn)\n",
    "\n",
    "print(f'fall-out: { my_fall_out(a, b):.4f}')\n",
    "\n",
    "# complete the code for f1-score\n",
    "def my_f1_score(y_real, y_predicted):\n",
    "  rcl = my_recall(y_real, y_predicted)\n",
    "  prc = my_precision(y_real, y_predicted)\n",
    "  return 2 * rcl * prc / (rcl + prc)\n",
    "\n",
    "print(f'f-score: {my_f1_score(a, b):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4omvWpxpTJ0R"
   },
   "source": [
    "Now, we can compare our functions with the corresponding measures from the `sklearn.metrics` module.\n",
    "\n",
    "To calculate *specificity* using `recall_score`, we can simply reverse (negate) the class labels—treating $0$ as the positive label instead of $1$. This lets recall measure the proportion of true negatives correctly identified.\n",
    "\n",
    "Try setting `pos_label=0` in `recall_score` to compute specificity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vcQVtAwpnUXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My accuracy:        0.6000   sklearn accuracy:   0.6000\n",
      "My recall:          0.8000   sklearn recall:     0.8000\n",
      "My specificity:     0.4000   sklearn specificity:0.4000\n",
      "My precision:       0.5714   sklearn precision:  0.5714\n",
      "My F-score:         0.6667   sklearn F-score:    0.6667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "print(f'{\"My accuracy:\":20}{my_accuracy(a, b):.4f}   {\"sklearn accuracy:\":20}{accuracy_score(a,b):.4f}')\n",
    "print(f'{\"My recall:\":20}{my_recall(a, b):.4f}   {\"sklearn recall:\":20}{recall_score(a,b):.4f}')\n",
    "print(f'{\"My specificity:\":20}{my_specificity(a, b):.4f}   {\"sklearn specificity:\":20}{recall_score(a,b, pos_label=0):.4f}')\n",
    "print(f'{\"My precision:\":20}{my_precision(a, b):.4f}   {\"sklearn precision:\":20}{precision_score(a,b):.4f}')\n",
    "print(f'{\"My F-score:\":20}{my_f1_score(a, b):.4f}   {\"sklearn F-score:\":20}{f1_score(a,b):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RewvvvSUkRD"
   },
   "source": [
    "## Evaluating of predictions on the titanic data\n",
    "### Data processing\n",
    "As we defined the prediction performance measures, we can proceed to data processing. First, we start with filling the mising values by using the [`fillna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) method. As the `Age` column is a numerical variable, we can replace the missing values using the median. Notice, that we use the `skipna` as a parameter for the median method, otherwise it would return a `NaN` as the data contains `NaN` values. For the binary `Embarked` column, we can fill in all mising values with the most frequent data. As the `Cabin` data is not very informative for us, we will drop it. Finally, we check if there are any missing values left.\n",
    "\n",
    "## Evaluating Predictions on the Titanic Data\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "Now that we've defined our prediction performance measures, we can proceed to data processing.\n",
    "\n",
    "- **Fill missing values:**  \n",
    "  - For the numerical `Age` column, replace missing values using the median with the `fillna` method. Be sure to use the `skipna=True` parameter with the median to avoid getting `NaN` due to missing data.\n",
    "  - For the categorical `Embarked` column, fill missing values with the most frequent entry (mode).\n",
    "- **Drop non-informative columns:**  \n",
    "  - Since `Cabin` does not provide useful information for our analysis, drop this column from the dataset.\n",
    "- **Check for remaining missing values:**  \n",
    "  - After processing, verify that no missing values remain.\n",
    "\n",
    "For more details on the `fillna` method and its usage, refer to the [pandas.DataFrame.fillna documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_XQ6sgXsuOmO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data counts:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "Missing data counts:\n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "Survived       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# copy the data\n",
    "train_data = train_df.copy()\n",
    "\n",
    "# replaces Age column with median\n",
    "train_data.fillna({'Embarked':train_data['Embarked'].value_counts().idxmax()}, inplace=True)\n",
    "train_data.fillna({'Age':train_data[\"Age\"].median(skipna=True)}, inplace=True)\n",
    "train_data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "print(f\"Missing data counts:\\n{train_data.isnull().sum()}\")\n",
    "\n",
    "# copy the data\n",
    "test_data = test_df.copy()\n",
    "\n",
    "# replaces Age column with median\n",
    "test_data.fillna({'Embarked':test_data['Embarked'].value_counts().idxmax()}, inplace=True)\n",
    "test_data.fillna({'Age':test_data[\"Age\"].median(skipna=True)}, inplace=True)\n",
    "test_data.fillna({'Fare':test_data[\"Fare\"].median(skipna=True)}, inplace=True)\n",
    "test_data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "print(f\"Missing data counts:\\n{test_data.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEyPUfqPU-tj"
   },
   "source": [
    "In the next step, we create a new column that indicates whether a passenger traveled alone. To do this, we combine the `SibSp` and `Parch` columns into a single column called `TravelAlone`.\n",
    "\n",
    "- If both `SibSp` and `Parch` are $0$, it means the passenger was alone.\n",
    "- Otherwise, the passenger was traveling with family.\n",
    "\n",
    "Once `TravelAlone` has been created, we remove the original `SibSp` and `Parch` columns from the datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "nRW-gxsJU_CS"
   },
   "outputs": [],
   "source": [
    "train_data['TravelAlone'] = np.where( (train_data[\"SibSp\"]+train_data[\"Parch\"]) > 0, 0, 1)\n",
    "train_data.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "\n",
    "test_data['TravelAlone'] = np.where( (test_data[\"SibSp\"]+test_data[\"Parch\"]) > 0, 0, 1)\n",
    "test_data.drop(['SibSp', 'Parch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QvYX-9TJzEn"
   },
   "source": [
    "Next, we apply one-hot encoding to the categorical columns to convert them into numerical format suitable for machine learning models. After encoding, we drop the columns that are not expected to influence the predictions (such as `Name`, `Ticket`, and `PassengerId`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LTUODmNUJzlt"
   },
   "outputs": [],
   "source": [
    "final_train=pd.get_dummies(train_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\n",
    "final_train.drop(['PassengerId', 'Name', 'Ticket'] , axis=1, inplace=True)\n",
    "\n",
    "final_test=pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\n",
    "final_test.drop(['PassengerId', 'Name', 'Ticket'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9TkOuVBJ3B3"
   },
   "source": [
    "Now, apply the same data processing steps to the test data.\n",
    "Additionally, you need to replace the missing values in `Fare` column with the median. As the last step verify that no missing values remain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sa3ikQmKg6M"
   },
   "outputs": [],
   "source": [
    "# do the same for the test data\n",
    "#test_data = test_df.copy()\n",
    "# Replace missing values, including the fare column\n",
    "# drop Cabin column\n",
    "# create TravelAlone column and drop the 'SibSp' and 'Parch' columns\n",
    "# onehot encode column\n",
    "# drop the 'PassengerId', 'Name' and 'Ticket' columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd-GmhKNKr4S"
   },
   "source": [
    "Below, we extract the feature matrix (input variables) and the target variable (output) from the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "pKG0uOSkKsVc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>TravelAlone</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age     Fare  TravelAlone  Pclass_1  Pclass_2  Pclass_3  Embarked_C  \\\n",
       "0  22.0   7.2500            0     False     False      True       False   \n",
       "1  38.0  71.2833            0      True     False     False        True   \n",
       "2  26.0   7.9250            1     False     False      True       False   \n",
       "3  35.0  53.1000            0      True     False     False       False   \n",
       "4  35.0   8.0500            1     False     False      True       False   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Sex_female  Sex_male  \n",
       "0       False        True       False      True  \n",
       "1       False       False        True     False  \n",
       "2       False        True        True     False  \n",
       "3       False        True        True     False  \n",
       "4       False        True       False      True  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = final_train.loc[:, final_train.columns != 'Survived']\n",
    "y_train = final_train[\"Survived\"]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMAMWJw7Kyag"
   },
   "source": [
    "Now, extract the feature matrix and target variable from the test data in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Ll-CRjRtKxX_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>TravelAlone</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.5</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age     Fare  TravelAlone  Pclass_1  Pclass_2  Pclass_3  Embarked_C  \\\n",
       "0  34.5   7.8292            1     False     False      True       False   \n",
       "1  47.0   7.0000            0     False     False      True       False   \n",
       "2  62.0   9.6875            1     False      True     False       False   \n",
       "3  27.0   8.6625            1     False     False      True       False   \n",
       "4  22.0  12.2875            0     False     False      True       False   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Sex_female  Sex_male  \n",
       "0        True       False       False      True  \n",
       "1       False        True        True     False  \n",
       "2        True       False       False      True  \n",
       "3       False        True       False      True  \n",
       "4       False        True        True     False  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = final_test.loc[:, final_test.columns != 'Survived']\n",
    "y_test = final_test[\"Survived\"]\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoDZMsoPPJJF"
   },
   "source": [
    "### KNN Classification and Feature Scaling\n",
    "\n",
    "Next, we will predict whether a passenger survived using the K-Nearest Neighbors (KNN) algorithm.\n",
    "\n",
    "**Why do we need feature scaling?**  \n",
    "When features are measured in different units or have different scales, KNN can be dominated by features with larger values. This happens because KNN relies on distance calculations—features with larger ranges will contribute more to the distance, potentially overshadowing smaller-scale features.\n",
    "\n",
    "To prevent this issue, we need to scale the features. A common approach is to normalize all features to the same range, such as $[0, 1]$, using techniques like Min-Max scaling or standardization.\n",
    "\n",
    "**Do we need to scale the target variable $y$?**  \n",
    "No. The target variable $y$ (survival: $0$ or $1$) is already a binary categorical variable and does not require scaling. Only the feature matrix $X$ needs to be scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0kb2t7-6PJpG"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scaler = MinMaxScaler()\n",
    "titan_data_norm = {\n",
    "    'y_train': y_train,\n",
    "    'X_train': X_scaler.fit_transform(X_train),\n",
    "    'y_test': y_test,\n",
    "    'X_test': X_scaler.transform(X_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fmj6kiXQR3xQ"
   },
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "In this section, we perform predictions using the K-Nearest Neighbors (KNN) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4_dEtRggR3bW"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkJI62v7SUTM"
   },
   "source": [
    "We will use the KNN classifier with $k=5$ neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wnz71LsaSU1e"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LancJ2pCSlxM"
   },
   "source": [
    "Fit the KNN classifier to the normalized training data in `titan_data_norm` and compute predictions for both the training and test datasets. Note that the `_est` suffix stands for \"estimate\" and refers to the predicted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HtR765XFSorr"
   },
   "outputs": [],
   "source": [
    "knn.fit(titan_data_norm[\"X_train\"], titan_data_norm[\"y_train\"])\n",
    "\n",
    "y_train_est = knn.predict(titan_data_norm[\"X_train\"])\n",
    "y_test_est = knn.predict(titan_data_norm[\"X_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiThihcZaWdF"
   },
   "source": [
    "To evaluate the model, we first compute confusion matrices for both the training and test data using the `confusion_matrix` function from `sklearn.metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "t60xpeYBEnsn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion matrix:\n",
      "[[269  73]\n",
      " [ 52 497]]\n",
      "Train confusion matrix:\n",
      "[[123  29]\n",
      " [ 52 214]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train confusion matrix:\\n{confusion_matrix(y_train, y_train_est, labels = [1, 0])}\")\n",
    "print(f\"Train confusion matrix:\\n{confusion_matrix(y_test, y_test_est, labels = [1, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXhOjaJqd1uh"
   },
   "source": [
    "Check the values of the performance measures you computed above for the test data. Your results should match the following:\n",
    "\n",
    "- **Accuracy**: 0.8062\n",
    "- **Recall**: 0.8092\n",
    "- **Specificity**: 0.8045\n",
    "- **Precision**: 0.7029\n",
    "- **Fall-out**: 0.1955\n",
    "- **F-score**: 0.7523\n",
    "\n",
    "If your values differ, review your confusion matrix calculation and ensure you're using the correct formulas for each metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "YyPTBxCSd694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8062\n",
      "recall: 0.8092\n",
      "specificity: 0.8045\n",
      "precision: 0.7029\n",
      "fall-out: 0.1955\n",
      "f-score: 0.7523\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: { my_accuracy(y_test, y_test_est):.4f}')\n",
    "print(f'recall: { my_recall(y_test, y_test_est):.4f}')\n",
    "print(f'specificity: {my_specificity(y_test, y_test_est):.4f}')\n",
    "print(f'precision: {my_precision(y_test, y_test_est):.4f}')\n",
    "print(f'fall-out: {my_fall_out(y_test, y_test_est):.4f}')\n",
    "print(f'f-score: {my_f1_score(y_test, y_test_est):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8Ze7qN7xkS6"
   },
   "source": [
    "Based on the results above, answer the following questions:\n",
    "\n",
    "1. What do you think may have caused the relatively low value of fall-out (0.1955)?\n",
    "2. Is a high fall-out a sign of a good classifier? Why or why not?\n",
    "\n",
    "Write your answer in the text cell below before proceeding to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TLGlsIuxkry"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgAi5epnkG7I"
   },
   "source": [
    "Now, refer to the documentation for the [Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) (MCC) in `sklearn`. Import the function and compute its value for the test data predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq7aGgbgkGnT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDe2oTLRn7E5"
   },
   "source": [
    "Did you get a value around $0.6$? If not, verify the following:\n",
    "\n",
    "- The true values (`y_test`) are passed as the first argument\n",
    "- The predicted values (`y_test_est`) are passed as the second argument\n",
    "- You are using the test data (not the training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS30d99biJLp"
   },
   "source": [
    "### Probabilistic Forecasting\n",
    "\n",
    "To obtain probabilistic forecasts where the output corresponds to the probability of $y=1$, we will train a logistic regression model.\n",
    "\n",
    "Using the `predict_proba` method, we can extract probabilistic predictions (the probability that the outcome is $1$). To transform the probabilistic output into a binary outcome, we apply a threshold $\\theta$:\n",
    "\n",
    "- If $y_p > \\theta$, then $y = 1$\n",
    "- Otherwise, $y = 0$\n",
    "\n",
    "We create a set $\\Theta$ of possible threshold values ranging from $0.01$ to $0.99$, incremented by $0.02$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "St8r_dgb0NVj"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, penalty = None)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "thresh_range = np.linspace(0.01,0.99,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH8RJw2IlQb4"
   },
   "source": [
    "Using each threshold, we transform the probabilistic output into a binary prediction. This way, we obtain a different binary outcome for each $\\theta_i \\in \\Theta$.\n",
    "\n",
    "We identify $\\theta_f$ as the threshold corresponding to the highest F-score value. In the plot below, $\\theta_f$ is visualized as a vertical dashed blue line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85pFsF12lQPY"
   },
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# obtaining vector of f-score values\n",
    "f_score_vct = np.array([f1_score(y_test, y_pred_proba > thresh) for thresh in thresh_range])\n",
    "\n",
    "## finds the best value of the threshold\n",
    "best_threshold_fscore = thresh_range[np.argmax(f_score_vct)]\n",
    "print(f'max F1-score = {np.max(f_score_vct):.4f} at threshold = {best_threshold_fscore}')\n",
    "\n",
    "plt.plot(thresh_range, f_score_vct)\n",
    "plt.xlabel('threshold value', fontsize=14),\n",
    "plt.ylabel('F1-score value', fontsize=14)\n",
    "plt.axvline(best_threshold_fscore, ls='--', color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpbS25xRiWGt"
   },
   "source": [
    "Now, apply the same approach to find the optimal threshold for the Matthews correlation coefficient (MCC). Your goal is to find $\\theta_{\\text{MCC}}$, the threshold $\\theta_i$ that maximizes the MCC.\n",
    "\n",
    "Complete the following two subtasks:\n",
    "\n",
    "1. Create a list comprehension to compute the MCC for each threshold in $\\Theta$, storing the results in a list\n",
    "2. Identify the threshold $\\theta_{\\text{MCC}}$ from `mcc_vct` that yields the highest MCC value and assign it to `best_threshold_mcc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jOM4WEHffG8"
   },
   "outputs": [],
   "source": [
    "# add the list comprehension for obtaining a list of Matthew's coefficients for the given threshold\n",
    "mcc_vct = np.array([ _your_code_here for thresh in _your_code_here_])\n",
    "\n",
    "# find the best value of the threshold for MCC\n",
    "best_threshold_mcc =\n",
    "print(f'max MCC = {np.max(mcc_vct):.4f} at threshold = {best_threshold_mcc}')\n",
    "\n",
    "plt.plot(thresh_range, mcc_vct)\n",
    "plt.xlabel('threshold value', fontsize=14),\n",
    "plt.ylabel('MCC value', fontsize=14)\n",
    "plt.axvline(best_threshold_mcc, ls='--', color='blue')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLfBcWCOH6Uv"
   },
   "source": [
    "In the cell below, apply the optimal threshold $\\theta_{\\text{MCC}}$ to convert the probabilistic predictions to binary outcomes using the following rule:\n",
    "\n",
    "$$\\text{if } y_p > \\theta_{\\text{MCC}} \\text{ then } y=1, \\text{ otherwise } y=0$$\n",
    "\n",
    "Next, compute the confusion matrix and accuracy using this optimal threshold value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTNORBs2H7Dv"
   },
   "outputs": [],
   "source": [
    "best_thresh_y = y_pred_proba > # add code\n",
    "# compute confusion matrix and accuracy (use the best threshold value)\n",
    "print(f'Confusion matrix: \\n{confusion_matrix( your_code_here, labels = [1, 0])}')\n",
    "print(f'Accuracy: {my_accuracy(your_code_here)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA9q7nfslPHQ"
   },
   "source": [
    "To gain a broader view of the classifier's performance across different threshold values, we will display the Receiver Operating Characteristic (ROC) curve along with the Area Under the Curve (AUC) for the probabilistic predictions.\n",
    "\n",
    "First, we extract the False Positive Rate (FPR) and True Positive Rate (TPR) values for each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITjQff15nPjp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thr = roc_curve(y_test, y_pred_proba)\n",
    "fpr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfvQX5iCnSwj"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='coral', label=f'ROC curve probabilistic (area = {auc(fpr, tpr):.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "plt.title('Receiver operating characteristic (ROC) curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkurd7dmnjH5"
   },
   "source": [
    "# Error Measures for Regression\n",
    "In this section, we will focus on error measures for regression problems. We will use the nonlinear dataset from a previous lab session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPls63vqni43"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgtlR7BXVac8"
   },
   "outputs": [],
   "source": [
    "#@title Loading the data { display-mode: \"form\" }\n",
    "np.random.seed(123)\n",
    "n = 400\n",
    "# creates a uniform sequence of n values between 0 and 8 (inclusive)\n",
    "X = np.linspace(0, 8, n)\n",
    "y = np.sin(X) + np.random.normal(0,0.4, n) + 0.2*X\n",
    "y = y.reshape(-1,1)\n",
    "X = X.reshape(-1,1)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"f(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LNAxZQHTro3"
   },
   "source": [
    "## Train-Validation-Test Split\n",
    "\n",
    "First, we need to perform a train-validation-test split of our data:\n",
    "\n",
    "- **Training set**: Used to train our models\n",
    "- **Validation set**: Used to evaluate models and select the best hyperparameter values\n",
    "- **Test set**: Used to report final error measures on unseen data\n",
    "\n",
    "While scikit-learn does not directly support a three-way split, we can achieve this by applying `train_test_split` twice.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Split the data into a *training* set and a *validation-test* set. In the next step, the validation-test set will be further split into *validation* and *test* sets. Use the data ratios specified in the code comments above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NixeogCQrTdH"
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "X_train, X_test_tmp, y_train, y_test_tmp = train_test_split(X, y, test_size= _your_code_here_  )# split the data into training and validation+test set\n",
    "\n",
    "X_val, X_test, y_val, y_test =  # split the test set into validation and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e0sStBG-MgD"
   },
   "source": [
    "Below, print the shapes of `X_train`, `X_val`, and `X_test`. Do they approximately correspond to the ratios specified above? If not, review your code and make corrections to ensure the data splits match the specified ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwnWTGrW-y5z"
   },
   "outputs": [],
   "source": [
    "print(f\"train:\\t\\t{X_train.shape[0]/X.shape[0]}\\nvalidation:\\t{X_val.shape[0]/X.shape[0]}\\ntest:\\t\\t{X_test.shape[0]/X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEJgIcTcgtPk"
   },
   "source": [
    "### Squared Regression Errors\n",
    "\n",
    "To measure the error of regression models, common metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and the coefficient of determination ($R^2$). All of these measures are included in the `sklearn` library.\n",
    "\n",
    "We can examine the relationship between MSE and $R^2$ using the training data. Below you will see code to compute MSE and $R^2$.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Calculate by what percentage the MSE of the regression model improved compared to the MSE of a dummy mean model (baseline that always predicts the mean)\n",
    "2. Do you notice any similarity to $R^2$?\n",
    "3. Write your explanation of how MSE and $R^2$ are related in the text cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9YGKgHPmHIQ"
   },
   "outputs": [],
   "source": [
    "lr_model = LinearRegression().fit(X_train, y_train)\n",
    "dummy_mean = DummyRegressor().fit(X_train, y_train)\n",
    "y_fit_train = lr_model.predict(X_train)\n",
    "y_dummy = dummy_mean.predict(X_train)\n",
    "\n",
    "mse_dummy_train = mean_squared_error(y_train, y_dummy)\n",
    "print(f\"Dummy MSE: {mse_dummy_train:.2f}\")\n",
    "\n",
    "# compute MSE\n",
    "mse_lin_reg_train = mean_squared_error(y_train, y_fit_train) # compute MSE\n",
    "print(f\"Linear model MSE: {mse_lin_reg_train:.2f}\")\n",
    "\n",
    "rmse_lin_reg_train = # calculate rmse\n",
    "print(f\"Linear model RMSE: {rmse_lin_reg_train:.2f}\")\n",
    "\n",
    "# obtain the r2_score for the linear model\n",
    "print(f\"R2 (linear model): {r2_score(y_train, y_fit_train):.4f}\")\n",
    "\n",
    "# compute by how many percent the MSE of linear regression improved its fit\n",
    "# compared to MSE of the mean  - write your code instead of mse_proportion\n",
    "print(f\"R2 calculated from MSE {1 - your_mse_proportion_here :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUSPxOOnkKic"
   },
   "source": [
    "Print also the test RMSE and decide whether the model is over or underfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7a6CtTXmI5M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgY_X_ldjtAv"
   },
   "source": [
    "### Percentage Error\n",
    "\n",
    "In regression tasks, you can use percentage errors to express the error measure in relative terms (without units).\n",
    "\n",
    "**Mean Absolute Percentage Error (MAPE)** has the following equation:\n",
    "\n",
    "$$MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\left|\\frac{y-\\hat{y}}{y}\\right| \\times 100\\%$$\n",
    "\n",
    "**Important note:** When $y$ is close to zero, MAPE will reach very high values. Therefore, for such applications, we recommend alternative measures such as [scaled errors](https://otexts.com/fpp3/accuracy.html#scaled-errors).\n",
    "\n",
    "Below, calculate MAPE for the linear regression model on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "error",
     "timestamp": 1763980353591,
     "user": {
      "displayName": "Milan Straka",
      "userId": "13527477701716738802"
     },
     "user_tz": -60
    },
    "id": "w2axYqOZjscB",
    "outputId": "9fcec4d4-12e7-42c4-fca1-37f383236c66"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ipython-input-2200881196.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2200881196.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    mape_lin_reg_train = # MAPE on train data\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mape_dummy_train = mean_absolute_percentage_error(y_train, y_dummy)\n",
    "print(f\"Dummy MAPE : {100*mape_dummy_train:.2f}%\")\n",
    "\n",
    "# compute MAPE for linear model\n",
    "mape_lin_reg_train = # MAPE on train data\n",
    "print(f\"Linear model MAPE : {100*mape_lin_reg_train:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vWDTMa_mOAx"
   },
   "source": [
    "Below, calculate and print the MAPE for both the validation and test sets. Is the MAPE much higher than on the training set? If so, what might this indicate about your model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uH1oc1umNY8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc522iMdpeOw"
   },
   "source": [
    "# K-Fold Cross-Validation\n",
    "\n",
    "As you have seen, the train versus test error largely depends on the hyperparameters (e.g., degree of polynomial, number of neighbors $k$, depth of a decision tree, etc.).\n",
    "\n",
    "In this section, we will apply cross-validation to find the best values of hyperparameters while avoiding overfitting. To demonstrate this, we will use the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKodLSa9pnK2"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qbnnx2vXXTGI"
   },
   "source": [
    "Below, we implement cross-validation. Note that we use negative MSE as the scoring metric because scikit-learn's model evaluation framework is built such that **higher return values are better than lower return values**.\n",
    "\n",
    "Since MSE is a loss metric (lower is better), it is negated to align with this convention. Therefore, all the error values will be negative, so we take their absolute values before averaging them across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG60N3y6XStE"
   },
   "outputs": [],
   "source": [
    "k_range = range(3, 60, 3)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    # creates a knn regression\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    # performs the crossvalidation and obtain the absolute value of the error\n",
    "    loss = abs(cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "    # take an average of each fold\n",
    "    k_scores.append(loss.mean())\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzvvhGzzEO9j"
   },
   "source": [
    "After inspecting the plot, identify and print the value of $K$ (number of neighbors) corresponding to the lowest cross-validated MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQJbN2G3EkYt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjjULo2jaLT9"
   },
   "source": [
    "---\n",
    "**Extra Task #1**\n",
    "\n",
    "---\n",
    "\n",
    "Perform a similar cross-validation with 5 folds using a KNN **classifier** on the train-test split **Titanic** dataset. Your goal is to find the value of $k$ in KNN that achieves the lowest test error.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Apply 5-fold cross-validation on the training set\n",
    "2. Evaluate different values of $k$ using an appropriate classification metric\n",
    "3. Select the best $k$ based on cross-validation results\n",
    "4. Train the final model with the optimal $k$ and evaluate on the test set\n",
    "\n",
    "To see the available classification metrics, check this [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). Consider using a metric beyond simple accuracy (e.g., F1-score, ROC AUC, Matthews correlation coefficient).\n",
    "\n",
    "Finally, print:\n",
    "- The optimal value of $k$\n",
    "- The training score\n",
    "- The test score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiMoU8QQaKX_"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR1kRWExpmfU"
   },
   "source": [
    "---\n",
    "**Extra Task #2**\n",
    "\n",
    "---\n",
    "\n",
    "Complete the cross-validation code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KF62on5ppxv"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "nobs = 150\n",
    "nfold = 5\n",
    "\n",
    "# function to shuffle the indicies of folds\n",
    "def cv_fold_indicies(n_obs, n_fold):\n",
    "  # get a sequence of fold indicies\n",
    "  fold_indicies = np.repeat(np.arange(0,n_fold), math.ceil(n_obs/n_fold)) [:n_obs]\n",
    "  # shuffle the fold indicies\n",
    "  shuffled_fold_indicies = np.random.choice(fold_indicies, size = n_obs, replace = False)\n",
    "  return shuffled_fold_indicies\n",
    "\n",
    "def my_cv(X, y, n_fold, model, metric, **kwargs):\n",
    "  # gets the folds\n",
    "  cv_indicies = cv_fold_indicies(len(y), n_fold)\n",
    "  # creates the result matrix filled with zeroes\n",
    "  result_matrix = np.zeros((n_fold,2))\n",
    "  for i in range(n_fold):\n",
    "    # obtains the train indicies as the indicies that are not i\n",
    "    train_indices = cv_indicies != i\n",
    "    # obtain the test cv_indicies\n",
    "    test_indices = # similar to the one above...\n",
    "    model.fit(X[train_indices,:], y[train_indices])\n",
    "    y_train = model.predict(X[train_indices,:])\n",
    "    # get predictions on the test fold\n",
    "    y_test = # your code here\n",
    "    train_metric = metric(y[train_indices], y_train)\n",
    "    # calculate value of test metric\n",
    "    test_metric = # your code here\n",
    "    result_matrix[i,] = (train_metric, test_metric)\n",
    "  return np.mean(result_matrix, axis = 0)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOQ1ITGkObuWkGVPn/twj72",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
